{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wget\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "from zipfile import ZipFile\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://bhichallenge.med.auth.gr/sites/default/files/ICBHI_final_database/ICBHI_final_database.zip'\n",
    "wget.download(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('ICBHI_final_database.zip', 'r') as zipObj:\n",
    "    # Extract all the contents of zip file in current directory\n",
    "    zipObj.extractall()\n",
    "os.remove('ICBHI_final_database.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'ICBHI_final_database/'\n",
    "filenames = [s.split('.')[0] for s in os.listdir(path = root) if '.txt' in s]\n",
    "# Just patient info\n",
    "filenames = filenames[:920]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Annotation_Data(file_name, root):\n",
    "    tokens = file_name.split('_')\n",
    "    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n",
    "    recording_annotations = pd.read_csv(os.path.join(root, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n",
    "    return (recording_info, recording_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_list = []\n",
    "rec_annotations = []\n",
    "rec_annotations_dict = {}\n",
    "for s in filenames:\n",
    "    (i,a) = Extract_Annotation_Data(s, root)\n",
    "    i_list.append(i)\n",
    "    rec_annotations.append(a)\n",
    "    rec_annotations_dict[s] = a\n",
    "recording_info = pd.concat(i_list, axis = 0)\n",
    "recording_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_label_list = []\n",
    "crack_list = []\n",
    "wheeze_list = []\n",
    "both_sym_list = []\n",
    "filename_list = []\n",
    "for f in filenames:\n",
    "    d = rec_annotations_dict[f]\n",
    "    no_labels = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 0)].index)\n",
    "    n_crackles = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 0)].index)\n",
    "    n_wheezes = len(d[(d['Crackles'] == 0) & (d['Wheezes'] == 1)].index)\n",
    "    both_sym = len(d[(d['Crackles'] == 1) & (d['Wheezes'] == 1)].index)\n",
    "    no_label_list.append(no_labels)\n",
    "    crack_list.append(n_crackles)\n",
    "    wheeze_list.append(n_wheezes)\n",
    "    both_sym_list.append(both_sym)\n",
    "    filename_list.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_df = pd.DataFrame(data = {'filename':filename_list, 'no label':no_label_list, 'crackles only':crack_list, 'wheezes only':wheeze_list, 'crackles and wheezees':both_sym_list})\n",
    "file_label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_df.iloc[919, file_label_df.columns.get_loc('filename')] = \"226_1b1_Pl_sc_Meditron\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_df.insert(1, 'id', recording_info['Patient number'].tolist())\n",
    "file_label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_url = 'https://bhichallenge.med.auth.gr/sites/default/files/ICBHI_final_database/ICBHI_challenge_train_test.txt'\n",
    "train_test_df = pd.read_table(split_url, delimiter='\\t', names=['filename', 'set'])\n",
    "train_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_label_df = file_label_df.merge(train_test_df, 'left')\n",
    "file_label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only = file_label_df[file_label_df['set']=='train']\n",
    "train_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_only.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients ID list, you can choose different ones\n",
    "val_id = ['112','132','138','163','166','221']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = train_only.loc[(train_only.id.isin(val_id))]\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_only.drop(val_df.index.values, axis=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = file_label_df[file_label_df['set']=='test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of respiratory cycle lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_list = []\n",
    "for i in range(len(rec_annotations)):\n",
    "    current = rec_annotations[i]\n",
    "    duration = current['End'] - current['Start']\n",
    "    duration_list.extend(duration)\n",
    "\n",
    "duration_list = np.array(duration_list)\n",
    "plt.hist(duration_list, bins = 50)\n",
    "print('longest cycle:{:.2f}'.format(max(duration_list)))\n",
    "print('shortest cycle:{:.2f}'.format(min(duration_list)))\n",
    "print('mean cycle length:{:.2f}'.format(np.mean(duration_list)))\n",
    "threshold = np.mean(duration_list)\n",
    "print('Number of samples less than {:.2f} seconds:{:.2f}'.format(threshold,\n",
    "                                                           np.sum(duration_list < threshold)))\n",
    "\n",
    "print('Number of samples higher than {:.2f} seconds:{:.2f}'.format(threshold,\n",
    "                                                           np.sum(duration_list > threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to split each individual sound file into separate sound clips containing one respiratory cycle each\n",
    "#output: [filename, (sample_data:np.array, start:float, end:float, crackles:bool(float), wheezes:bool(float)) (...) ]\n",
    "import math\n",
    "import librosa\n",
    "\n",
    "def read_wav_file(str_filename, target_rate):\n",
    "    data, sample_rate = librosa.load(str_filename, sr=None)\n",
    "        \n",
    "    if (sample_rate != target_rate):\n",
    "        data = librosa.resample(y=data, orig_sr=sample_rate, target_sr=target_rate)\n",
    "    \n",
    "    return (target_rate, data.astype(np.float32))\n",
    "\n",
    "def slice_data(start, end, raw_data,  sample_rate):\n",
    "    max_ind = len(raw_data) \n",
    "    start_ind = min(int(start * sample_rate), max_ind)\n",
    "    end_ind = min(int(end * sample_rate), max_ind)\n",
    "    return raw_data[start_ind: end_ind]\n",
    "\n",
    "def get_sound_samples(recording_annotations, file_name, root, sample_rate):\n",
    "    sample_data = [file_name]\n",
    "    (rate, data) = read_wav_file(os.path.join(root, file_name + '.wav'), sample_rate)\n",
    "    \n",
    "    for i in range(len(recording_annotations.index)):\n",
    "        row = recording_annotations.loc[i]\n",
    "        start = row['Start']\n",
    "        end = row['End']\n",
    "        crackles = row['Crackles']\n",
    "        wheezes = row['Wheezes']\n",
    "        audio_chunk = slice_data(start, end, data, rate)\n",
    "        sample_data.append((audio_chunk, start, end, crackles, wheezes))\n",
    "    return sample_data\n",
    "\n",
    "#Fits each respiratory cycle into a fixed length audio clip, splits may be performed and zero padding is added if necessary\n",
    "#original:(arr,c,w) -> output:[(arr,c,w),(arr,c,w)]\n",
    "def split_and_pad(original, desiredLength, sampleRate):\n",
    "    output_buffer_length = int(desiredLength * sampleRate)\n",
    "    soundclip = original[0]\n",
    "    n_samples = len(soundclip)\n",
    "    total_length = n_samples / sampleRate #length of cycle in seconds\n",
    "    n_slices = int(math.ceil(total_length / desiredLength)) #get the minimum number of slices needed\n",
    "    samples_per_slice = n_samples // n_slices\n",
    "    src_start = 0 #Staring index of the samples to copy from the original buffer\n",
    "    output = [] #Holds the resultant slices\n",
    "    for i in range(n_slices):\n",
    "        src_end = min(src_start + samples_per_slice, n_samples)\n",
    "        length = src_end - src_start\n",
    "        copy = generate_padded_samples(soundclip[src_start:src_end], output_buffer_length)\n",
    "        output.append((copy, original[1], original[2]))\n",
    "        src_start += length\n",
    "    return output\n",
    "\n",
    "def generate_padded_samples(source, output_length):\n",
    "    copy = np.zeros(output_length, dtype = np.float32)\n",
    "    src_length = len(source)\n",
    "    if(output_length > src_length):\n",
    "        #tile forward sounds to fill empty space\n",
    "        cursor = 0\n",
    "        while(cursor + src_length) < output_length:\n",
    "            copy[cursor:(cursor + src_length)] = source[:]\n",
    "            cursor += src_length\n",
    "    else:\n",
    "        copy[:src_length] = source[:]\n",
    "    #\n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a list of respiratory cycles, and splits and pads each cycle into fixed length buffers (determined by desiredLength(seconds))\n",
    "#Then takes the split and padded sample and transforms it into a mel spectrogram\n",
    "\n",
    "def apply_split_and_pad(original, desiredLength, sampleRate):\n",
    "    output = []\n",
    "    for d in original:\n",
    "        \n",
    "        lst_result = split_and_pad(d, desiredLength, sampleRate) #Time domain\n",
    "        output.extend(lst_result)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility used to import all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_samples(filenames, annotation_dict, root, target_rate, desired_length):\n",
    "    cycle_list = []\n",
    "    for file in filenames:\n",
    "        data = get_sound_samples(annotation_dict[file], file, root, target_rate)\n",
    "        cycles_with_labels = [(d[0], d[3], d[4]) for d in data[1:]]\n",
    "        cycle_list.extend(cycles_with_labels)\n",
    "    \n",
    "    #Sort into respective classes\n",
    "    no_labels = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 0))]\n",
    "    c_only = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 0))] \n",
    "    w_only = [c for c in cycle_list if ((c[1] == 0) & (c[2] == 1))]\n",
    "    c_w = [c for c in cycle_list if ((c[1] == 1) & (c[2] == 1))]\n",
    "    \n",
    "    #Split up cycles into sound clips with fixed lengths so they can be fed into a CNN\n",
    "    \n",
    "    none  = (apply_split_and_pad(no_labels, desired_length, target_rate))\n",
    "    \n",
    "    c = (apply_split_and_pad(c_only, desired_length, target_rate))\n",
    "    \n",
    "    w = (apply_split_and_pad(w_only, desired_length, target_rate))\n",
    "    \n",
    "    c_w = (apply_split_and_pad(c_w, desired_length, target_rate)) \n",
    "    \n",
    "    dict = {'none':none,'crackles':c,'wheezes':w, 'both':c_w}\n",
    "\n",
    "    return dict\n",
    "\n",
    "# Function return number of samples \n",
    "def print_sample_count(src_dict):\n",
    "    print('none:{}\\ncrackles:{}\\nwheezes:{}\\nboth:{}'.format(len(src_dict['none']),\n",
    "                                                        len(src_dict['crackles']),\n",
    "                                                        len(src_dict['wheezes']),\n",
    "                                                        len(src_dict['both'])))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Samples per set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "target_sample_rate = 32000\n",
    "sample_length_seconds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train \n",
    "sample_dict_train = extract_all_samples(train_df.filename.tolist(), rec_annotations_dict, root, target_sample_rate, sample_length_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "sample_dict_test = extract_all_samples(test_df.filename.tolist(), rec_annotations_dict, root, target_sample_rate, sample_length_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation\n",
    "sample_dict_val = extract_all_samples(val_df.filename.tolist(), rec_annotations_dict, root, target_sample_rate, sample_length_seconds) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Samples Available')\n",
    "print('[Training set]')\n",
    "print_sample_count(sample_dict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Samples Available')\n",
    "print('[Testing set]')\n",
    "print_sample_count(sample_dict_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Samples Available')\n",
    "print('[Validation set]')\n",
    "print_sample_count(sample_dict_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving wav files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Train samples\n",
    "dir = 'ICBHI/Train'\n",
    "os.makedirs(dir, exist_ok=True)\n",
    "train_list = []\n",
    "\n",
    "for sample in ['none', 'crackles', 'wheezes', 'both']:\n",
    "    # Saving Normal Samples\n",
    "    for i, file in enumerate(sample_dict_train[sample]):\n",
    "        name = sample+\"_\"+str(i)+\".wav\"\n",
    "        x = file[0]\n",
    "        path = os.path.join(dir, name)\n",
    "        sf.write(path, x, target_sample_rate)\n",
    "\n",
    "        if sample=='none':\n",
    "            train_dict = {'filename':name, 'id':0}\n",
    "        elif sample=='crackles':\n",
    "            train_dict = {'filename':name, 'id':1}\n",
    "        elif sample=='wheezes':\n",
    "            train_dict = {'filename':name, 'id':2}\n",
    "        else:\n",
    "            train_dict = {'filename':name, 'id':4}\n",
    "            \n",
    "        train_list.append(train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for train samples and save It\n",
    "train_samples = pd.DataFrame(train_list, columns = train_dict.keys())\n",
    "train_samples.to_csv(dir+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Test samples\n",
    "test_dir = 'ICBHI/Test'\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "test_list = []\n",
    "\n",
    "for sample in ['none', 'crackles', 'wheezes', 'both']:\n",
    "    for i, file in enumerate(sample_dict_test[sample]):\n",
    "        name = sample+\"_\"+str(i)+\".wav\"\n",
    "        x = file[0]\n",
    "        path = os.path.join(test_dir, name)\n",
    "        sf.write(path, x, target_sample_rate)\n",
    "        \n",
    "        if sample=='none':\n",
    "            test_dict = {'filename':name, 'id':0}\n",
    "        elif sample=='crackles':\n",
    "            test_dict = {'filename':name, 'id':1}\n",
    "        elif sample=='wheezes':\n",
    "            test_dict = {'filename':name, 'id':2}\n",
    "        else:\n",
    "            test_dict = {'filename':name, 'id':4}\n",
    "        \n",
    "        test_list.append(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for Test samples and save It \n",
    "test_samples = pd.DataFrame(test_list, columns = test_dict.keys())\n",
    "test_samples.to_csv(test_dir+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Validation samples\n",
    "val_dir = 'ICBHI/Val'\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "val_list = []\n",
    "\n",
    "# Saving Normal Samples\n",
    "for sample in ['none', 'crackles', 'wheezes', 'both']:\n",
    "    for i, file in enumerate(sample_dict_val[sample]):\n",
    "        name = sample+\"_\"+str(i)+\".wav\"\n",
    "        x = file[0]\n",
    "        path = os.path.join(val_dir, name)\n",
    "        sf.write(path, x, target_sample_rate)\n",
    "        \n",
    "        if sample=='none':\n",
    "            val_dict = {'filename':name, 'id':0}\n",
    "        elif sample=='crackles':\n",
    "            val_dict = {'filename':name, 'id':1}\n",
    "        elif sample=='wheezes':\n",
    "            val_dict = {'filename':name, 'id':2}\n",
    "        else:\n",
    "            val_dict = {'filename':name, 'id':4}\n",
    "        \n",
    "        val_list.append(val_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataframe for validation samples nd save It \n",
    "val_samples = pd.DataFrame(val_list, columns = val_dict.keys())\n",
    "val_samples.to_csv(val_dir+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('ICBHI_final_database')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tcc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b1f3976d9f1d9d0fd208cc8ca0ec295ceaf3eae61f9be2a9afbe480971bda05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
